# -*- coding: utf-8 -*-
"""MT19049_Akanksha_Code1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12cw1yubFpVu8qdppCSIxZ5DpCVjB514G

Import Files
"""

#import csv file
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('Tweets.csv', encoding = "ISO-8859-1")
import nltk 
import string 
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')
nltk.download('stopwords')
import re 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.tokenize import sent_tokenize, word_tokenize 
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import precision_recall_fscore_support
nltk.download('punkt')
data.head()

"""Data Size"""

data.shape

"""Total attributes in data"""

data.columns

"""Data Cleanning

1.checking null and missing values.
"""



data.isnull().sum()

"""Calculate in percentage of null values"""

((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)

"""Columns deleted with more then 90% null values and unrequired attributes"""

del data['tweet_coord']
del data['airline_sentiment_gold']
del data['negativereason_gold']
#deleted on the basis of domain knowledge
del data['airline_sentiment_confidence']
del data['negativereason_confidence']
del data['retweet_count']
del data['user_timezone']

data['tweet_location'].value_counts()
#as per my analysis i already have i dont need this attribute because i just wanna analyse tweets airlines wise
#so i'll delete it
del data['tweet_location']

#we dont need twiiter id and name of that specific person who twitted  because we just need twit countes of airlines
del data['tweet_id']
del data['name']

data['tweet_created'].value_counts()
#as we already know that this data is taken at year 2015 so i will analyse that at 2015 how many reasons and all stuff 
#are taken out on 2015year
del data['tweet_created']

"""After deleting irrelevant columns"""

data.head()

#https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe
((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)

"""Fill remaining null values of negativereason column with "no feedback""""

#https://www.geeksforgeeks.org/python-pandas-dataframe-fillna-to-replace-null-values-in-dataframe/
data['negativereason'].fillna("No feedback", inplace = True)

data.head()

"""Cleanned Data"""

((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)



"""Data Exploration"""

data['airline_sentiment'].value_counts()

data['negativereason'].value_counts()

data['airline'].value_counts()

data['text'].value_counts()

data.columns

total_sentiment_counts=data['airline_sentiment'].value_counts()
total_sentiment_counts

data.shape

labels = 'negative', 'neutral', 'positive'
colors = ['gold', 'yellowgreen', 'lightcoral']
explode = (0, 0, 0)  # explode 1st slice
 
# Plot
plt.pie(total_sentiment_counts, explode=explode, labels=labels, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)
 
plt.axis('equal')
plt.tight_layout()
plt.show()

#https://www.kaggle.com/rashmitarouy01/us-airlines-twitter-sentiment-analysis

air_senti=pd.crosstab(data.airline, data.airline_sentiment)
air_senti

total_negativereasons_counts=data['negativereason'].value_counts()

data.negativereason.value_counts().plot(kind='pie', autopct='%1.0f%%')

air_nega=pd.crosstab(data.negativereason, data.airline_sentiment)
air_nega

airline_nega=pd.crosstab(data.airline, data.negativereason)
airline_nega

total_airlines_tweet_counts=data['airline'].value_counts()
total_airlines_tweet_counts

data.airline.value_counts().plot(kind='pie', autopct='%1.0f%%')

#https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/
airline_sentiment = data.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()
airline_sentiment.plot(kind='bar')

airline_sentiment = data.groupby(['airline', 'negativereason']).airline_sentiment.count().unstack()
airline_sentiment.plot(kind='bar')

fig, ax = plt.subplots(figsize=(15,7))
data.groupby(['airline']).count()['negativereason'].plot(ax=ax)

#https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/
airline_sentiment = data.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()
airline_sentiment.plot(kind='bar')

airline_sentiment_negative = data.groupby(['airline', 'negativereason']).airline_sentiment.count().unstack()
airline_sentiment_negative.plot(kind='bar')

"""Module for preprocessing"""

#https://stackoverflow.com/questions/43646877/python-extract-positive-words-from-a-string-using-sentiment-vader
#https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/

def text_preprocess(text):
     #remove links
    unigram = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text2 = re.sub("[^a-zA-Z]", " ",unigram) 
    unigram=text2.lower();
    unigram = re.sub(r'\d+', '',unigram)
    #remove size 2 words
    unigram=re.sub(r'\b\w{1,2}\b', '',unigram )
    #unigram=re.sub(regex, '@airline',text, flags=re.IGNORECASE)
    stop_words = set(stopwords.words("english")) 
    meaningful_words = [w for w in unigram if not w in stop_words]
    #remove punctuations
    translator = str.maketrans('', '', string.punctuation) 
    unigram=unigram.translate(translator)
    #remove white spaces
    unigram=" ".join(unigram.split())
    word_tokens = word_tokenize(unigram) 
    unigram = [word for word in word_tokens if word not in stop_words] 
    return len(unigram)
# https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas
# https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe

data['new_clean_tweet_len']=""

#https://stackoverflow.com/questions/53986877/pandas-iterate-over-a-row-and-adding-the-value-to-an-empty-column
p=[]
for index, row in data.iterrows():
    p.append(text_preprocess(row['text']))
data['new_clean_tweet_len']=p

#https://stackoverflow.com/questions/43646877/python-extract-positive-words-from-a-string-using-sentiment-vader
#https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/
def text_preprocess(text):
     #remove links
    unigram = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text2 = re.sub("[^a-zA-Z]", " ",unigram) 
    unigram=text2.lower();
    unigram = re.sub(r'\d+', '',unigram)
#     remove size 2 words
    unigram=re.sub(r'\b\w{1,2}\b', '',unigram )
    #unigram=re.sub(regex, '@airline',text, flags=re.IGNORECASE)
    stop_words = set(stopwords.words("english")) 
    meaningful_words = [w for w in unigram if not w in stop_words]
    #make upper to lower evry words
   
    #remove numbers
   
    #remove punctuations
    translator = str.maketrans('', '', string.punctuation) 
    unigram=unigram.translate(translator)
   
    #remove white spaces
    unigram=" ".join(unigram.split())
    #remove stop words
   
    word_tokens = word_tokenize(unigram) 
    unigram = [word for word in word_tokens if word not in stop_words]    
    return unigram
# https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas
# https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe
data['new_clean_tweet']=""

#https://stackoverflow.com/questions/53986877/pandas-iterate-over-a-row-and-adding-the-value-to-an-empty-column
p1=[]
for index, row in data.iterrows():
    p1.append(text_preprocess(row['text']))
data['new_clean_tweet']=p1

def text_preprocess_unigram(text):
#     Tokenization
# Removing stop words
# Stemming
# Transform the tokens back to one string
    
# https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/
# # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/
#  convert everything to words and split them
    letters_only = re.sub("[^a-zA-Z]", " ",text)
    words = letters_only.lower().split()     
#     print(words)remove stop words
    stops = set(stopwords.words("english"))                  
#     meaningful_words = [w for w in words if not w in stops]
# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/
    meaningful_words=[]
    for w in words: 
        if w not in stops: 
            meaningful_words.append(w) 
#     meaningful_words=[]
    return str(meaningful_words) 
    

    
data['unigram_tweet']=data['text'].apply(lambda x: text_preprocess_unigram(x))

"""Module to make bigram"""

def text_preprocess_bigram(text):
#     Tokenization
# Removing stop words
# Stemming
# Transform the tokens back to one string
    
# https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/
# # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/     
    bi_gram=[]
#  convert everything to words and split them
    letters_only = re.sub("[^a-zA-Z]", " ",text)
    words = letters_only.lower().split()     
#     print(words)remove stop words
    stops = set(stopwords.words("english"))                  
# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/
    meaningful_words=[]
    for w in words: 
        if w not in stops: 
            meaningful_words.append(w) 
    for l in nltk.bigrams(meaningful_words):
         bi_gram.append(' '.join( l ))
    for i in meaningful_words:
         bi_gram.append(i)
    return str(bi_gram) 
    

    
data['bigram_tweet']=data['text'].apply(lambda x: text_preprocess_bigram(x))

"""make three different classes 3 different numbers"""

def sentiments_to_binary(text):
    if text=='positive':
        return 1
    elif text=='neutral':
        return 2
    else:
        return 3


data['sentiment_in_binary']=""
p4=[]
for index, row in data.iterrows():
    p4.append(sentiments_to_binary(row['airline_sentiment']))
data['sentiment_in_binary']=p4

data.head()

"""Split Test and Train data"""

# 20 % for test set
train,test = train_test_split(data,test_size=0.2,random_state=42)

train_clean_tweet=[]
for t in train['bigram_tweet']:
    
    train_clean_tweet.append(t)
test_clean_tweet=[]
for t in test['bigram_tweet']:
    test_clean_tweet.append(t)

train_clean_tweet_unigram=[]
for t in train['unigram_tweet']:
    
    train_clean_tweet_unigram.append(t)
test_clean_tweet_unigram=[]
for t in test['unigram_tweet']:
    test_clean_tweet_unigram.append(t)

"""Feature Extraction"""

# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
v = CountVectorizer(analyzer = "word")
train_features= v.fit_transform(train_clean_tweet)
test_features=v.transform(test_clean_tweet)

plt.spy(train_features)

plt.spy(test_features)

"""Classifier:
1.KNN
"""

#https://towardsdatascience.com/building-a-k-nearest-neighbors-k-nn-model-with-scikit-learn-51209555453a
# https://www.programcreek.com/python/example/89260/sklearn.metrics.precision_recall_fscore_support
knn = KNeighborsClassifier(n_neighbors = 5)
# Fit the classifier to the data
knn.fit(train_features,train['sentiment_in_binary'])
pred=knn.predict(test_features)
Accuracy_knn=knn.score(test_features,test['sentiment_in_binary'])
p,r,f,_=precision_recall_fscore_support(test['sentiment_in_binary'],pred,average='macro')
print("p: ",p,"r: ",r,"f:",f)

bar_width=0.5
final=plt.bar(('Accuracy','Precision','Recall','F1-score'),[Accuracy_knn,p,r,f])
plt.xticks(('Accuracy','Precision','Recall','F1-score'),('Accuracy','Precision','Recall','F1-score'))
plt.ylabel('Value Score')
plt.title('Test Data measure')
plt.show()

"""2.Multinomial classifier"""

# https://www.kaggle.com/carlolepelaars/predicting-sentiment-with-ml-80-accuracy/comments
# multinomial classifier 
# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html
#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html
# https://www.programcreek.com/python/example/89260/sklearn.metrics.precision_recall_fscore_support
multi_nb = MultinomialNB()
multi_nb_u = MultinomialNB()
multi_nb.fit(train_features,train['sentiment_in_binary'])
prdict=multi_nb.predict(test_features)
p1,r1,f1,_=precision_recall_fscore_support(test['sentiment_in_binary'],prdict,average='macro')
print("p: ",p1,"r: ",r1,"f:",f1)

Accuracy_multinomial=multi_nb.score(test_features,test['sentiment_in_binary'])
bar_width=0.5
final=plt.bar( ('Accuracy','Precision','Recall','F1-score'),[Accuracy_multinomial,p1,r1,f1])
plt.xticks( ('Accuracy','Precision','Recall','F1-score'), ('Accuracy','Precision','Recall','F1-score'))
plt.ylabel('Value Score')
plt.title('Test Data measure')
plt.show()

"""Model comparision"""

print("KNN classifier")
print("Accuracy: ",Accuracy_knn*100)
print("precision: ",p*100,"recall: ",r*100,"f-score:",f*100)
print("multinomial classifier")
print("Accuracy : ",Accuracy_multinomial*100)
print("precision: ",p1*100,"recall: ",r1*100,"f-score:",f1*100)









